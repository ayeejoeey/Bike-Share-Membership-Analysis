# Preparation

------------------------------------------------------------------------

## Libraries

```{r}
library(readr)
library(tibble)
library(tidyr)
library(tidyverse)
library(ggmice)
library(mice)
```

Import necessary data-sets\
*each data-set has new columns added 'ride_length' and 'day_of_week'*\
Past 12 months of bike trip data:

```{r}
jan_tripdata2023 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/jan-tripdata2023.csv")
dec_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/dec-tripdata2022.csv")
nov_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/nov-tripdata2022.csv")
oct_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/oct-tripdata2022.csv")
sept_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/sept-tripdata2022.csv")
aug_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/aug-tripdata2022.csv")
july_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/july-tripdata2022.csv")
june_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/june-tripdata2022.csv")
may_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/may-tripdata2022.csv")
april_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/april-tripdata2022.csv")
march_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/march-tripdata2022.csv")
feb_tripdata2022 <- read_csv("Local-repo-only/Cyclistic-tripdata-past12months/feb-tripdata2022.csv")
```

Combining all data-sets into one data-frame:

```{r}
DF = do.call("rbind", list(jan_tripdata2023,dec_tripdata2022,nov_tripdata2022,oct_tripdata2022,sept_tripdata2022,aug_tripdata2022,july_tripdata2022,june_tripdata2022,may_tripdata2022,april_tripdata2022,march_tripdata2022,feb_tripdata2022))
```

```{r}
head(DF)
```

##### These next steps will be to further explore the missing values throughout the data, to determine how to handle the null values

```{r}
# To start, I will execute some basic code to get an idea of how many missing values are in the data

# This code counts the total number of missing values in the dataframe
sum(is.na(DF))
```

```{r}
# To get a better idea of how
plot_pattern(DF)
```

```{r}
# This next line of code shows me a percentage of missing values within each column of the dataframe
round(colSums(is.na(DF))/ nrow(DF),2)
```

```{r}
# Visualizing the missing data may help to understand the overall picture as well as any patterns between the missing values

# This line of code creates a plot by using the 'plot_pattern' function from the ggmice' package
plot_pattern(DF)
```

##### Before dealing with the missing values, I want to get a better idea of how the missing values correlate with the observed values.

##### I noticed that the majority of the start/end longitude and latitude values are not missing. Which leads me to believe that I can potentially pair the geo location with the '(start/end)\_station_name'.

```{r}

unique_values_end_station <- unique(DF$end_station_name)
```

```{r}
view(unique_values_end_station)
```

```{r}
length(unique_values_end_station)
```

```{r}
# This line of code filters the data frame to show only the rows with certain latitude and longitude conditions

DF_Unique1 <- DF %>%
  group_by(start_station_name)%>%
  filter(start_lat == 41.80000 & start_lng == -87.62000)
```

```{r}
# This line of code displays a table of 'start_station_name' values that pair with the latitude and longitude condition within the "DF_Unique1" filter

table(DF_Unique1$start_station_name, useNA = "ifany")
```

##### For this lat & long combination, I can replace the 'start_station_name' values with either "Calumet Ave & 51st St" or "Public Rack - Calumet Ave & 51st St" because they are the most common station names.

##### However, after running the code below, you'll see that there are over 600 thousand unique latitude values. Which makes it unrealistic to manually sift through each latitude/longitude value to find the start_station_names that are paired with them.

```{r}
length(unique(DF$start_lat))
```

##### Moving forward, to simplify this dataframe, I will just include the time, length, membership type and ride type feature columns as part of the analysis. This will be considered a limitation on the data analysis project as a whole. 

```{r}
# this code creates a new dataframe with only the selected feature columns
DF_revised <- DF%>%
  select(ride_id, rideable_type, started_at, ended_at, member_casual, ride_length, day_of_week)
```

##### Now that I've created a simplified and limited new dataframe, I'll take a look at the missing and observed values to see if further cleaning is needed.

```{r}
# this code creates a visual of missing values in the dataframe
plot_pattern(DF_revised)
```

##### After reviewing the missing values and noticing that there are only 856 missing values out of 5.7m entries, deleting these rows with missing values will not have impact on the overall analysis. However, I think it is important to understand why there are missing values to begin with, considering that the "ride_length" column was formed out of simple calculation by subtracting "start_at" from "ended_at" columns.

```{r}
# to view the ride_length missing values on the dataframe I used to following code
DF_revised %>% filter(is.na(ride_length))
```

##### By filtering the null values in "ride_length" I was able to understand why the initial calculation was bringing up a null value. The calculation for "ride_length" brought up null values for either two reasons. One, the ride length extended 24hrs or two, the "start_at" time was actually after the "end_at" time, which would've resulted in a negative ride length time. 

##### Now that I understand why there are null values in "ride_length" and that I cannot replace the values without manipulating the data entries and jeopardizing the integrity of the data, I will just continue to remove these rows.

```{r}
DF_revised <- DF_revised[complete.cases(DF_revised),]
```

```{r}
print(DF_revised)
```
